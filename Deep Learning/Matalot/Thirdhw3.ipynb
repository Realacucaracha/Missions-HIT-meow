{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\color{pink}{Universal - Approximation - Theorem}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes::\n",
    "\n",
    "I used the previous Proof for sigmoid and a proof for ReLu from the internet for help.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://math.uchicago.edu/~may/REU2018/REUPapers/Guilhoto.pdf\n",
    "\n",
    "-page 11, Lemma3.15. TheReLUfunctionis1-discriminatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ${\\color{pink}{Theorem}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let f : [a,b] → R be a continuous function, where [a,b] ⊂ R. For any ϵ > 0, \n",
    "\n",
    "there exists a feedforward neural network with a single hidden layer and ReLu function such that sup x∈[a,b] $$ |f(x) −  \\hat{f}(x)| < ϵ$$\n",
    "where  f^(x) is the output of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\color{pink}{Proof:}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLu Activation Function: $$ \\textnormal{ReLu} : R → [0,\\infty) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\textnormal{ReLu}=\\left\\{\\begin{array}{lll}\n",
    "                x & , & x>0 \\\\\n",
    "                0 & , & x\\le 0\n",
    "            \\end{array}\\right.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ${\\color{pink}{Step 2: Function - Decomposition:}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to construct a sigmoid bounded, continiuous function from subtracting two ReLU functions with different parameters,\n",
    "$$\n",
    "\\textnormal{Therefore, we are going to use this function:}\\\\$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textnormal{f(x)}=\\left\\{\\begin{array}{lll}\n",
    "                0 & , & x<0 \\\\\n",
    "                x & , & xϵ[0,1] \\\\\n",
    "                1 & , & x>1\n",
    "            \\end{array}\\right.\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then any function of the form \n",
    "$$ \n",
    "g(x)= f( yx+\\theta )  ,  y \\neq 0 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g(x) = Relu(yx - \\theta_1) - Relu(yx - \\theta_2) \n",
    "$$\n",
    "\n",
    "by setting:\n",
    "$$\n",
    "\\theta_1 = - \\theta /y ,\n",
    "$$\n",
    "$$\n",
    "\\theta_2 = (1 - \\theta) /y\n",
    "$$\n",
    "\n",
    "if y=0 then:\n",
    "$$\n",
    "g(x) = f(\\theta) = \n",
    "\\begin{align*}\n",
    "=\\left\\{\\begin{array}{lll}\n",
    "                Relu(f(\\theta)) & , & f(\\theta)>0 \\\\\n",
    "                -Relu(-f(\\theta)) & , & f(\\theta) \\le 0\n",
    "            \\end{array}\\right.\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to use :\n",
    "$$\n",
    "g(x) = Relu(yx - \\theta_1) - Relu(yx - \\theta_2)\n",
    "$$\n",
    "$$\n",
    "\\lim_{x\\to -\\infty} g(x) = 0 ,,,, \n",
    "\\lim_{x\\to \\infty} g(x) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let f(x) be a continuous function defined on a compact interval [a,b]. By the Stone-Weierstrass Theorem, f(x) can be uniformly approximated by a finite linear combination of basis functions:\n",
    "$$\n",
    "f = \\sum_{i=0}^{N} c_i \\cdot \\phi_i\n",
    "$$\n",
    "where ϕi(x) are continuous functions and ci ∈ R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ${\\color{pink}{Step 3: ReLu - Approximates - Indicator - Functions}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function can approximate an indicator function:\n",
    "$$\n",
    "q_{[x_0,x_1]}(x) =\n",
    "\\begin{align*}\n",
    "\\left\\{\\begin{array}{lll}\n",
    "                1 & , & x ϵ [x_0,x_1] \\\\\n",
    "                0 & , & \\textnormal{else}\n",
    "            \\end{array}\\right.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For large values of w, the function g(wx + b) transitions sharply at x = −b/w:\n",
    "\n",
    "$$\n",
    "g(wx +b) = \n",
    "\\begin{align*}\n",
    "\\left\\{\\begin{array}{lll}\n",
    "                1 & , & x >b/w \\\\\n",
    "                0 & , & x \\le b/w\n",
    "            \\end{array}\\right.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ${\\color{pink}{Step 4: Constructing - the - Neural - Network}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a finite sum of ReLu functions, we approximate f(x) as:\n",
    "\n",
    "$$  \\hat{f}(x) = \\sum_{i=0}^{N} c_i \\cdot g(w_i x + b_i) $$ \n",
    "\n",
    "where w_i, b_i, and c_i are learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ${\\color{pink}{Step 5: Error - Bound}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the approximation error as: $$E(x) = |f(x)− ˆ f(x)|.$$ By the uniform continuity of f(x) and the compactness of [a,b], for any ϵ > 0, there exist parameters wi, bi, and ci such that: sup x∈[a,b] Conclusion E(x) < ϵ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ${\\color{pink}{Conclusion!}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E(x) < ϵ. Thus,\n",
    "any continuous function on a compact interval [a,b] can be approximated arbitrarily closely by a single-layer neural network with a ReLu activation function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
